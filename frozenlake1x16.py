# -*- coding: utf-8 -*-
"""FrozenLake1x16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BzxVNa86ggAogzTZWr3DT31136aQmE7p

Reinforcement learning avec réseaux de neuronnes

Paramètres
"""

total_episodes = 1         # Total episodes
# learning_rate = 0.5           # Learning rate
max_steps = 99                # Max steps per episode
gamma = 0.95                  # Discounting rate

# Exploration parameters
epsilon = 0.8                 # Exploration rate # au début explore 

# max_epsilon = 0.99             # Exploration probability at start # 5% d'exploration
min_epsilon = 0.2            # Minimum exploration probabilit
# decay_rate = 0.005             # Exponential decay rate for exploration prob 
decay_rate = 0.00000015

load_weight = True
save_weight = False  

load_path = "/content/drive/MyDrive/frozen_lake_weight/1x16_normal/"
save_path = "/content/drive/MyDrive/frozen_lake_weight/test/"

slippery = False

"""### Import des librairies

Utile pour l'environnement
"""

import numpy as np
import gym
import tensorflow as tf
import random

"""Utile pour l'affichage graphique """

!apt-get install -y xvfb python-opengl > /dev/null 2>&1
!pip install gym pyvirtualdisplay > /dev/null 2>&1
!pip install gym[classic_control]

import gym
import numpy as np

import matplotlib.pyplot as plt

from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

"""### Création de l'environnement

- le FrozenLake-V0 est depreceated pour la version de gym utilisé
- l'option "is_slippery" autorise le déplacement diagonal
- l'option "map_name" permet de choisir entre deux version "4x4" ou "8x8"
"""

# env = gym.make("FrozenLake-v1", map_name="4x4", is_slippery=False)
# env = gym.make("FrozenLake-v1", render_mode = "ansi", is_slippery=False)
env = gym.make("FrozenLake-v1", is_slippery=slippery)

"""On veut connaitre nb_etats et nb_actions, qui seront respectivement le nombre d'input du réseau de neuronne et le nombre d'output du réseau de neuronne """

nb_actions = env.action_space.n
nb_etats = env.observation_space.n

"""### Création du réseau de neuronne

Caractéristiques du réseau de neuronne :
- tf_input_size : nombre d'entrée dans la couche input du réseau 
- tf_hidden_size : nombre de neuronne dans la couche hidden du réseau
- tf_output_size : nombre de neuronne dans la couhe sortie du réseau
"""

tf_input_size = nb_etats
tf_output_size = nb_actions
tf_hidden_size = (tf_input_size+tf_output_size)//2
# tf_hidden_size = int(2/3*tf_input_size+tf_output_size)

tf.compat.v1.reset_default_graph() # pour supprimer les variables tensorflow d'avant
tf.compat.v1.disable_eager_execution() # nécessaire pour utiliser compat.v1

# Placeholder du réseau
tf_inputs = tf.compat.v1.placeholder(tf.float32, [None, tf_input_size]) # (1x16)
tf_next_q = tf.compat.v1.placeholder(tf.float32, [None, tf_output_size]) # (1x4)

weight_initer = tf.compat.v1.truncated_normal_initializer(mean=1., stddev=0.1)
biases_initer = tf.compat.v1.truncated_normal_initializer(mean=1., stddev=0.1)


# Hidden Layers (16x10) (10)
# tf.compat.v1.zeros_initializer
tf_weights_1 = tf.compat.v1.get_variable("tf_weights_1", [tf_input_size, tf_hidden_size], initializer = weight_initer)
tf_biases_1 = tf.compat.v1.get_variable("tf_biases_1", [tf_hidden_size], initializer=biases_initer)
tf_outputs_1 = tf.nn.relu(tf.matmul(tf_inputs, tf_weights_1) + tf_biases_1)
# tf_outputs_1 = tf.matmul(tf_inputs, tf_weights_1) + tf_biases_1

# Output (10x4) (4)
tf_weights_out = tf.compat.v1.get_variable("tf_weights_out", [tf_hidden_size, tf_output_size], initializer=weight_initer)
tf_biases_out = tf.compat.v1.get_variable("tf_biases_out", [tf_output_size], initializer=biases_initer)
tf_outputs = tf.matmul(tf_outputs_1, tf_weights_out) + tf_biases_out
# tf_outputs = tf.sigmoid(tf.matmul(tf_outputs_1, tf_weights_out) + tf_biases_out)

# Fonctions
tf_action = tf.argmax(input=tf_outputs, axis=1)
tf_loss = tf.reduce_sum(input_tensor=tf.square(tf_outputs - tf_next_q))
# tf_optimize = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.001).minimize(tf_loss)
tf_optimize = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01).minimize(tf_loss)

"""Pour faire fonctionner le réseau, on peut utiliser les sessions de tensorflow. <br>
Les fonctions utilisables sont :
- tf_outputs : (feed : tf_inputs) 
- tf_action : (feed : tf_inputs) pour obtenir le meilleur tf_action possible
- tf_loss : (feed : tf_inputs, tf_next_q) pour calculer la différence entre les outputs calculés par le réseau et les outputs idéals (donnée par le q_learning)
- tf_optimize : (feed : tf_outputs, tf_next_q) appel le tf_loss pour changer les poids afin de minimaliser le loss

Poids à l'initialisation
"""

sess = tf.compat.v1.InteractiveSession()
initializer = tf.compat.v1.global_variables_initializer()
sess.run(initializer)

import sys 
print("tf_weights_1", tf_weights_1)
sess.run(tf.print(tf_weights_1, output_stream=sys.stderr))
print("\n")
print("tf_biases_1", tf_biases_1)
sess.run(tf.print(tf_biases_1, output_stream=sys.stderr))
print("\n")
print("tf_weights_out", tf_weights_out)
sess.run(tf.print(tf_weights_out, output_stream=sys.stderr))
print("\n")
print("tf_biases_out", tf_biases_out)
sess.run(tf.print(tf_biases_out, output_stream=sys.stderr))
print("\n")

for i in range(16):
  print("Etat : {}, {}".format(i,sess.run([tf_outputs], feed_dict={tf_inputs: np.identity(16)[i:i+1]})[0][0]))

"""### Entrainement du réseau"""

X = []
eps = 1
# rate = 0.00002
rate = 0.00000015
mini = 0.2
for i in range(10000):
  eps = mini + (eps - mini)*np.exp(-rate*i)
  X.append(eps)
print(X)
import matplotlib.pyplot as plt
plt.plot(X)

"""Sauvegarde des poids """

saver = tf.compat.v1.train.Saver()
if load_weight:
    saver.restore(sess, load_path)
    print("Model restored.")

rewards = []

for episode in range(total_episodes):
    state = env.reset()
    done = False
    total_rewards = 0
    old_state = -10
    # on fait commencer aléatoirement sur la map
    # state = random.randint(0,15)
    ## print("episode {} commence le jeu au state {} ".format(episode, state))
    ## reward = 0
    for step in range(max_steps):
        reward = 0
        # choix de l'action
        # politique espilon-greedy
        exploration = random.uniform(0,1)
        if exploration > epsilon:
            action = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[state:state+1]})[0][0]
        else:
            action = env.action_space.sample()
            if state in [2,3]:
              action = random.choice([0,1,2])
            if state in [7,11]:
              action = random.choice([0,1,3])
            if state in [13,14]:
              action = random.choice([0,2,3])
            if state in [4,8]:
              action = random.choice([1,2,3])
            if state == 0:
              action = random.choice([1,2])
            if state == 3:
              action = random.choice([0,1])
            if state == 12:
              action = random.choice([2,3])
            if state == 15:
              action = random.choice([0,3])
            ## print("explore")
        
        

        # on avance   
        # new_state, reward, done, info = env.step(action)
        new_state, _, done, info = env.step(action)
        
        # print("old_state {},new_state {}, action {}".format(state,new_state,action))
        if new_state in [5,7,11,12]:
            reward -= 10000
        """
        if new_state in [5,7,11,12]:
            print("done?",done,"new_state",new_state)
        """

        """
        if new_state == 15:
            reward += 100
        if old_state == new_state :
            reward -= 100
        """
        if new_state == state:
            reward -= 1000
        if old_state == new_state :
            reward -= 1000
        if new_state == 15:
            reward += 10000

        # rewarding and updating
        q_vector = sess.run([tf_outputs], feed_dict={tf_inputs: np.identity(16)[state:state+1]})[0][0]
        # print("state {} / q_vector {} / action {}".format(state,q_vector,action))
        # q_target = reward + gamma * np.max(q_vector) # correspond en principe à l'action choisie
        q_target = reward + np.max(q_vector)
        q_vector[action] = q_target 
        # print("q_vector {} / q_target {}".format(q_vector,q_target))
        _, loss = sess.run([tf_optimize, tf_loss],
                          feed_dict={tf_inputs: np.identity(16)[state:state+1], 
                                      tf_next_q:q_vector.reshape(1,4)})
        
        old_state = state
        state = new_state
        total_rewards += reward 
        if (step > 97):
            print("Episode {} finished after {} timesteps, last state : {} and total_rewards {}".format(episode, step+1,state,total_rewards))
            epsilon = min_epsilon + (epsilon - min_epsilon)*np.exp(-decay_rate*episode)
            break
        if done == True:
            print("Episode {} finished after {} timesteps, last state : {} and total_rewards {}".format(episode, step+1,state,total_rewards))
            epsilon = min_epsilon + (epsilon - min_epsilon)*np.exp(-decay_rate*episode)
            break
    rewards.append(total_rewards)
    # print(total_rewards)
    print("episode {} finished".format(episode))
# print ("Score over time: " +  str(sum(rewards)/total_episodes))

saver = tf.compat.v1.train.Saver()
if save_weight:
    saved_path = saver.save(sess, save_path)
    print("Model saved in file: %s" % saved_path)

for i in range(16):
  print("Etat : {}, {}".format(i,sess.run([tf_outputs], feed_dict={tf_inputs: np.identity(16)[i:i+1]})[0][0]))

tab = np.zeros((4,4))

tab[0][0] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[0:0+1]})[0][0]
tab[0][1] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[1:1+1]})[0][0]
tab[0][2] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[2:2+1]})[0][0]
tab[0][3] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[3:3+1]})[0][0]

tab[1][0] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[4:4+1]})[0][0]
tab[1][1] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[5:5+1]})[0][0]
tab[1][2] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[6:6+1]})[0][0]
tab[1][3] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[7:7+1]})[0][0]

tab[2][0] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[8:8+1]})[0][0]
tab[2][1] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[9:9+1]})[0][0]
tab[2][2] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[10:10+1]})[0][0]
tab[2][3] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[11:11+1]})[0][0]

tab[3][0] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[12:12+1]})[0][0]
tab[3][1] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[13:13+1]})[0][0]
tab[3][2] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[14:14+1]})[0][0]
tab[3][3] = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[15:15+1]})[0][0]

print(tab)
print("0: LEFT, 1: DOWN, 2: RIGHT, 3: UP")

"""Poids après entrainement et utilisé pendant la phase de test"""

print("tf_weights_1", tf_weights_1)
sess.run(tf.print(tf_weights_1, output_stream=sys.stderr))
print("\n")
print("tf_biases_1", tf_biases_1)
sess.run(tf.print(tf_biases_1, output_stream=sys.stderr))
print("\n")
print("tf_weights_out", tf_weights_out)
sess.run(tf.print(tf_weights_out, output_stream=sys.stderr))
print("\n")
print("tf_biases_out", tf_biases_out)
sess.run(tf.print(tf_biases_out, output_stream=sys.stderr))
print("\n")

"""

```
# Ce texte est au format code
```

Affichage graphiqu"""

import time 


env = gym.make("FrozenLake-v1",is_slippery=slippery)
state = env.reset()
prev_screen = env.render(mode='rgb_array')
plt.imshow(prev_screen)

for episode in range(100):
  env.reset()
  for i in range(50):
    action = sess.run([tf_action], feed_dict={tf_inputs: np.identity(16)[state:state+1]})[0][0]
    print("Episode/Step {} {}".format(episode,i))
    print("State/Next_action {} {}".format(state,action))
    time.sleep(1)
    new_state, reward, done, info = env.step(action)
    screen = env.render(mode='rgb_array')
    
    plt.imshow(screen)
    ipythondisplay.clear_output(wait=True)
    ipythondisplay.display(plt.gcf())
    state = new_state
    if done:
      break
ipythondisplay.clear_output(wait=True)
env.close()

display.stop()